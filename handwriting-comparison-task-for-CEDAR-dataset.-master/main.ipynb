{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(file_path_input_diffn_pairs, file_path_input_same_pairs,human_observed_features):\n",
    "    data_input_diffn_pairs = []\n",
    "    data_input_same_pairs = []\n",
    "    human_observed_features_matrix = []\n",
    "    \n",
    "    with open(file_path_input_same_pairs, 'rU') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            dataRow = []\n",
    "            for column in row:\n",
    "                dataRow.append(column)\n",
    "            data_input_same_pairs.append(dataRow)\n",
    "    \n",
    "    with open(file_path_input_diffn_pairs, 'rU') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            dataRow = []\n",
    "            for column in row:\n",
    "                dataRow.append(column)\n",
    "            data_input_diffn_pairs.append(dataRow) \n",
    "\n",
    "    with open(human_observed_features, 'rU') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            dataRow = []\n",
    "            for column in row:\n",
    "                dataRow.append(column)\n",
    "            human_observed_features_matrix.append(dataRow) \n",
    "    \n",
    "    input_same_pairs = np.array(data_input_same_pairs)\n",
    "    input_diffn_pairs = np.array(data_input_diffn_pairs)\n",
    "    human_observed_features_list = np.array(human_observed_features_matrix)\n",
    "    \n",
    "    return input_same_pairs, input_diffn_pairs, human_observed_features_list\n",
    "\n",
    "# Partition the target vector as 80% of total for training\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    return t\n",
    "\n",
    "def GenerateTestingTarget(rawTraining,TrainingCount, TestingPercent = 10):\n",
    "    T_len = int(math.ceil(len(rawTraining)*0.01*TestingPercent))\n",
    "    V_End = TrainingCount + T_len\n",
    "    dataMatrix = rawTraining[TrainingCount+1:V_End]\n",
    "    return dataMatrix\n",
    "\n",
    "# Partition the data(input values) vector as 80% of total for training\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent):\n",
    "    T_len = int(math.ceil(len(rawData)*0.01*TrainingPercent))\n",
    "    d2 = rawData[:T_len]\n",
    "    return d2\n",
    "\n",
    "def GenerateValidationDataMatrix(rawData, TrainingCount, ValPercent = 10):\n",
    "    T_len = int(math.ceil(len(rawData)*0.01*ValPercent))\n",
    "    V_End = TrainingCount + T_len\n",
    "    dataMatrix = rawData[TrainingCount+1:V_End]\n",
    "    return dataMatrix\n",
    "\n",
    "def GenerateBigSigma(Data, MuMatrix,IsSynthetic):\n",
    "    DataT       = np.transpose(Data)\n",
    "    BigSigma    = np.zeros((len(DataT),len(DataT)))\n",
    "    varVect     = []\n",
    "    for i in range(0,len(np.transpose(Data[0]))):\n",
    "        vct = []\n",
    "        for j in range(0,1266):\n",
    "            vct.append(DataT[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(DataT)):\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    if IsSynthetic == True:\n",
    "        BigSigma = np.dot(3,BigSigma)\n",
    "    else:\n",
    "        BigSigma = np.dot(200,BigSigma)\n",
    "    return BigSigma\n",
    "\n",
    "# Intermediate calculations of Gaussian Radial basis function\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "# Calculation of Gaussian Radial basis function using it's vector form of formula\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingLen):\n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(Data[R], MuMatrix[C], BigSigInv)\n",
    "    return PHI\n",
    "\n",
    "# Calculation of weight vector using the derived form of original linear regression equation.\n",
    "def GetWeightsClosedForm(PHI, T, Lambda):\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    W           = np.dot(INTER, T)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    return Y\n",
    "\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(np.around(VAL_TEST_OUT[i], 0) == np.around(ValDataAct[i],0)):\n",
    "            counter+=1\n",
    "    accuracy = ((counter/len(VAL_TEST_OUT))*100)\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "\n",
    "def GradientDescent(W, TRAINING_PHI, VAL_PHI, TEST_PHI, output_data_training, output_data_validation, output_data_testing):\n",
    "    print(\"Entered GD\")\n",
    "    W_Now        = W\n",
    "    La           = 2\n",
    "    learningRate = 0.01\n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    L_Accuracy_TR = 0.0\n",
    "    L_Accuracy_Val = 0.0\n",
    "    L_Accuracy_Test = 0.0\n",
    "\n",
    "    for i in range(0,1000):\n",
    "        Delta_E_D     = -np.dot((output_data_training[i]- np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "        La_Delta_E_W  = np.dot(La,W_Now)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "        W_T_Next      = W_Now + Delta_W\n",
    "        W_Now         = W_T_Next\n",
    "\n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,output_data_training)\n",
    "        L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "        L_Accuracy_TR = float(Erms_TR.split(',')[0])\n",
    "\n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,output_data_validation)\n",
    "        L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "        L_Accuracy_Val = float(Erms_Val.split(',')[0])\n",
    "\n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,output_data_testing)\n",
    "        L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "        L_Accuracy_Test = float(Erms_Test.split(',')[0])\n",
    "    \n",
    "    print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "    print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "    print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "    print(\"Training accuracy = \" + str(L_Accuracy_TR))    \n",
    "    print(\"Validation accuracy = \" + str(L_Accuracy_Val))    \n",
    "    print(\"Testing accuracy = \" + str(L_Accuracy_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegressionMethod(input_data_training,input_data_validation,input_data_testing, output_data_training, output_data_validation, output_data_testing):\n",
    "    C_Lambda = 0.03\n",
    "\n",
    "    kmeans = KMeans(n_clusters=10, random_state=0).fit(input_data_training)\n",
    "    Mu = kmeans.cluster_centers_\n",
    "    \n",
    "    BigSigma     = GenerateBigSigma(input_data_training, Mu, True)\n",
    "    TRAINING_PHI = GetPhiMatrix(input_data_training, Mu, BigSigma, 1266)\n",
    "    VAL_PHI      = GetPhiMatrix(input_data_validation, Mu, BigSigma, 158)\n",
    "    TEST_PHI     = GetPhiMatrix(input_data_testing, Mu, BigSigma, 156) \n",
    "    W            = GetWeightsClosedForm(TRAINING_PHI,output_data_training,0.03)\n",
    "    GradientDescent(W, TRAINING_PHI, VAL_PHI, TEST_PHI, output_data_training, output_data_validation, output_data_testing)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def GetLogisticTarget(W,input_data):\n",
    "    \n",
    "    A_temp = np.dot(input_data, W)\n",
    "    A = sigmoid(A_temp)\n",
    "    return A\n",
    "    \n",
    "def LogisticRegressionOwnImpl(input_data_training, input_data_validation, input_data_testing, output_data_training, output_data_validation, output_data_testing):\n",
    "    \n",
    "    NUM_OF_EPOCHS = 1000\n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    LR = 0.0001\n",
    "    W = np.random.rand(len(np.transpose(input_data_training)), 1) #(18,)\n",
    "\n",
    "    for epoch in tqdm_notebook(range(NUM_OF_EPOCHS)):\n",
    "        \n",
    "        A = GetLogisticTarget(W, input_data_training)\n",
    "\n",
    "        Z = np.subtract(A,output_data_training)\n",
    "        DELTA_W = np.dot(np.transpose(input_data_training), Z)\n",
    "        W = W - np.dot(LR, DELTA_W)\n",
    "\n",
    "\n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "    TR_TEST_OUT   = np.dot(input_data_training,W) \n",
    "    Erms_TR       = GetErms(TR_TEST_OUT,output_data_training)\n",
    "    L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    L_Accuracy_TR = float(Erms_TR.split(',')[0])\n",
    "\n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "    VAL_TEST_OUT  = np.dot(input_data_validation,W) \n",
    "    Erms_Val      = GetErms(VAL_TEST_OUT,output_data_validation)\n",
    "    L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    L_Accuracy_Val = float(Erms_Val.split(',')[0])\n",
    "\n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "    TEST_OUT      = np.dot(input_data_testing,W) \n",
    "    Erms_Test = GetErms(TEST_OUT,output_data_testing)\n",
    "    L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "    L_Accuracy_Test = float(Erms_Test.split(',')[0])\n",
    "    \n",
    "    print (\"E_rms Training   = \" + str(np.around((L_Erms_TR),5)))\n",
    "    print (\"E_rms Validation = \" + str(np.around((L_Erms_Val),5)))\n",
    "    print (\"E_rms Testing    = \" + str(np.around((L_Erms_Test),5)))\n",
    "    print(\"Training accuracy = \" + str(L_Accuracy_TR))    \n",
    "    print(\"Validation accuracy = \" + str(L_Accuracy_Val))    \n",
    "    print(\"Testing accuracy = \" + str(L_Accuracy_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticMethod(input_data_training, input_data_validation, input_data_testing, output_data_training, output_data_validation, output_data_testing):\n",
    "    \n",
    "    logisticRegression = LogisticRegression()\n",
    "    logisticRegression.fit(input_data_training,output_data_training)\n",
    "\n",
    "    L_Erms_TR_logistic = []\n",
    "    L_Erms_VAL_logistic    = []\n",
    "    L_Erms_Test_logistic  = []\n",
    "    L_Accuracy_TR = 0.0\n",
    "    L_Accuracy_Val = 0.0\n",
    "    L_Accuracy_Test = 0.0\n",
    "\n",
    "    logistic_output_train_data = logisticRegression.predict(input_data_training)\n",
    "    Erms_train_logistic = GetErms(logistic_output_train_data,output_data_training)\n",
    "    L_Erms_TR_logistic.append(float(Erms_train_logistic.split(',')[1]))\n",
    "    L_Accuracy_Train = float(Erms_train_logistic.split(',')[0])\n",
    "\n",
    "    logistic_output_valid_data = logisticRegression.predict(input_data_validation)\n",
    "    # print(logistic_output_valid_data.shape)\n",
    "    Erms_valid_logistic = GetErms(logistic_output_valid_data,output_data_validation)\n",
    "    L_Erms_VAL_logistic.append(float(Erms_valid_logistic.split(',')[1]))\n",
    "    L_Accuracy_Valid = float(Erms_valid_logistic.split(',')[0])\n",
    "\n",
    "    logistic_output_test_data = logisticRegression.predict(input_data_testing)\n",
    "    Erms_Test_logistic = GetErms(logistic_output_test_data,output_data_testing)\n",
    "    L_Erms_Test_logistic.append(float(Erms_Test_logistic.split(',')[1]))\n",
    "    L_Accuracy_Test = float(Erms_Test_logistic.split(',')[0])\n",
    "\n",
    "    print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR_logistic),5)))\n",
    "    print (\"E_rms Validation = \" + str(np.around(min(L_Erms_VAL_logistic),5)))\n",
    "    print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test_logistic),5)))\n",
    "\n",
    "    print(\"Training Accuracy = \" + str(L_Accuracy_Train))\n",
    "    print(\"Validation Accuracy = \" + str(L_Accuracy_Valid))\n",
    "    print(\"Testing Accuracy  = \" + str(L_Accuracy_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "\n",
    "def GetAccuracyNeuralNetworks(processedTestingLabel, predictedTestLabel):\n",
    "        wrong   = 0\n",
    "        right   = 0\n",
    "\n",
    "        predictedTestLabelList = []\n",
    "\n",
    "        for i,j in zip(processedTestingLabel,predictedTestLabel):\n",
    "\n",
    "            if np.argmax(i) == j:\n",
    "                right = right + 1\n",
    "            else:\n",
    "                wrong = wrong + 1\n",
    "\n",
    "        print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "        print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))\n",
    "        \n",
    "def NeuralNetworkMethod(features, GSC_input_appended_features_shuffled_tr, \n",
    "                        GSC_input_appended_features_shuffled_test,\n",
    "                        GSC_input_appended_target_shuffled_tr,\n",
    "                        GSC_input_appended_target_shuffled_test):\n",
    "\n",
    "    NUM_HIDDEN_NEURONS_LAYER_1 = 200\n",
    "    LEARNING_RATE = 0.05\n",
    "    NUM_OF_EPOCHS = 1500\n",
    "    BATCH_SIZE = 128\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, features])\n",
    "    y = tf.placeholder(tf.float32, [None, 1])\n",
    "    \n",
    "    # Initializing the input to hidden layer weights\n",
    "    input_hidden_weights  = init_weights([features, NUM_HIDDEN_NEURONS_LAYER_1])\n",
    "    # Initializing the hidden to output layer weights\n",
    "    hidden_output_weights = init_weights([NUM_HIDDEN_NEURONS_LAYER_1, 1])\n",
    "\n",
    "    # Computing values at the hidden layer\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(x, input_hidden_weights))\n",
    "    # Computing values at the output layer\n",
    "    output_layer = tf.matmul(hidden_layer, hidden_output_weights)\n",
    "    \n",
    "    error_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=y))\n",
    "\n",
    "    prediction = tf.argmax(output_layer, 1)\n",
    "    \n",
    "    training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(error_function)\n",
    "    \n",
    "    training_accuracy = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for epoch in tqdm_notebook(range(NUM_OF_EPOCHS)):\n",
    "            \n",
    "            #Shuffle the Training Dataset at each epoch\n",
    "            p = np.random.permutation(range(len(GSC_input_appended_features_shuffled_tr)))\n",
    "            processedTrainingData  = GSC_input_appended_features_shuffled_tr[p]\n",
    "            processedTrainingLabel = GSC_input_appended_target_shuffled_tr[p]\n",
    "\n",
    "            # Start batch training\n",
    "            for start in range(0, len(GSC_input_appended_features_shuffled_tr), BATCH_SIZE):\n",
    "                end = start + BATCH_SIZE\n",
    "                sess.run(training, feed_dict={x: processedTrainingData[start:end], \n",
    "                                              y: processedTrainingLabel[start:end]})\n",
    "            # Training accuracy for an epoch\n",
    "            training_accuracy.append(np.mean(np.argmax(processedTrainingLabel, axis=1) ==\n",
    "                                 sess.run(prediction, feed_dict={x: processedTrainingData,\n",
    "                                                                 y: processedTrainingLabel})))\n",
    "        # Testing\n",
    "        predictedTestLabel = sess.run(prediction, feed_dict={x: GSC_input_appended_features_shuffled_test})\n",
    "        GetAccuracyNeuralNetworks(predictedTestLabel, GSC_input_appended_target_shuffled_tr)\n",
    "\n",
    "        return training_accuracy, predictedTestLabel\n",
    "\n",
    "def NeuralNetworkMethodGSC(features, GSC_input_appended_features_shuffled_tr, \n",
    "                        GSC_input_appended_features_shuffled_test,\n",
    "                        GSC_input_appended_target_shuffled_tr,\n",
    "                        GSC_input_appended_target_shuffled_test):\n",
    "\n",
    "    NUM_HIDDEN_NEURONS_LAYER_1 = 200\n",
    "    LEARNING_RATE = 0.05\n",
    "    NUM_OF_EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, features])\n",
    "    y = tf.placeholder(tf.float32, [None, 1])\n",
    "    \n",
    "    # Initializing the input to hidden layer weights\n",
    "    input_hidden_weights  = init_weights([features, NUM_HIDDEN_NEURONS_LAYER_1])\n",
    "    # Initializing the hidden to output layer weights\n",
    "    hidden_output_weights = init_weights([NUM_HIDDEN_NEURONS_LAYER_1, 1])\n",
    "\n",
    "    # Computing values at the hidden layer\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(x, input_hidden_weights))\n",
    "    # Computing values at the output layer\n",
    "    output_layer = tf.matmul(hidden_layer, hidden_output_weights)\n",
    "    \n",
    "    error_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=y))\n",
    "\n",
    "    prediction = tf.argmax(output_layer, 1)\n",
    "    \n",
    "    training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(error_function)\n",
    "    \n",
    "    training_accuracy = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for epoch in tqdm_notebook(range(NUM_OF_EPOCHS)):\n",
    "            \n",
    "            #Shuffle the Training Dataset at each epoch\n",
    "            p = np.random.permutation(range(len(GSC_input_appended_features_shuffled_tr)))\n",
    "            processedTrainingData  = GSC_input_appended_features_shuffled_tr[p]\n",
    "            processedTrainingLabel = GSC_input_appended_target_shuffled_tr[p]\n",
    "\n",
    "            # Start batch training\n",
    "            for start in range(0, len(GSC_input_appended_features_shuffled_tr), BATCH_SIZE):\n",
    "                end = start + BATCH_SIZE\n",
    "                sess.run(training, feed_dict={x: processedTrainingData[start:end], \n",
    "                                              y: processedTrainingLabel[start:end]})\n",
    "            # Training accuracy for an epoch\n",
    "            training_accuracy.append(np.mean(np.argmax(processedTrainingLabel, axis=1) ==\n",
    "                                 sess.run(prediction, feed_dict={x: processedTrainingData,\n",
    "                                                                 y: processedTrainingLabel})))\n",
    "        # Testing\n",
    "        predictedTestLabel = sess.run(prediction, feed_dict={x: GSC_input_appended_features_shuffled_test})\n",
    "        GetAccuracyNeuralNetworks(predictedTestLabel, GSC_input_appended_target_shuffled_tr)\n",
    "\n",
    "        return training_accuracy, predictedTestLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleDataSet(input_appended_features_same_matrix, input_appended_features_diffn_matrix, input_appended_features_target_same, input_appended_features_target_diffn):\n",
    "    input_size = len(input_appended_features_same_matrix)\n",
    "\n",
    "    input_appended_features_same_matrix_withTarget = np.concatenate((np.array(input_appended_features_same_matrix), input_appended_features_target_same.reshape(input_size,1)), axis=1)\n",
    "    input_appended_features_diffn_matrix_withTarget = np.concatenate((np.array(input_appended_features_same_matrix), input_appended_features_target_diffn.reshape(input_size,1)), axis=1)\n",
    "    \n",
    "    input_data_training_appended_withTarget = np.concatenate([input_appended_features_same_matrix_withTarget, input_appended_features_diffn_matrix_withTarget])\n",
    "    input_data_training_appended_withTarget_shuffled=np.random.shuffle(input_data_training_appended_withTarget)\n",
    "    \n",
    "    feature_size = len(np.transpose(input_data_training_appended_withTarget))-1\n",
    "    input_appended_features_target = input_data_training_appended_withTarget[:,[feature_size]] \n",
    "    input_appended_features = input_data_training_appended_withTarget[:,:-1]\n",
    "    \n",
    "    return input_appended_features, input_appended_features_target\n",
    "\n",
    "def partitionDataSet(input_appended_features_target_diffn, TrainingPercent):\n",
    "    \n",
    "    training_target_diffn = GenerateTrainingTarget(input_appended_features_target_diffn, TrainingPercent)\n",
    "    validation_target_diffn = GenerateTestingTarget(input_appended_features_target_diffn,len(training_target_diffn))\n",
    "    testing_target_diffn = GenerateTestingTarget(input_appended_features_target_diffn, (len(training_target_diffn)+len(validation_target_diffn)))\n",
    "    return training_target_diffn, validation_target_diffn, testing_target_diffn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Human Observed Features Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoMo\\Anaconda3\\envs\\ML Env\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: 'U' mode is deprecated\n",
      "  \n",
      "C:\\Users\\MoMo\\Anaconda3\\envs\\ML Env\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: 'U' mode is deprecated\n",
      "  \n",
      "C:\\Users\\MoMo\\Anaconda3\\envs\\ML Env\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    }
   ],
   "source": [
    "file_path_input_same_pairs = r'same_pairs.csv'\n",
    "file_path_input_diffn_pairs = r'diffn_pairs.csv'\n",
    "file_path_human_observed_features = r'HumanObserved-Features-Data.csv'\n",
    "\n",
    "# Raw data \n",
    "input_same_pairs, input_diffn_pairs, human_observed_features_extract = import_data(file_path_input_diffn_pairs, file_path_input_same_pairs, file_path_human_observed_features)\n",
    "\n",
    "# Deleting headings from raw data\n",
    "input_same_pairs = np.delete(input_same_pairs,0, 0)\n",
    "input_diffn_pairs = np.delete(input_diffn_pairs, 0, 0)\n",
    "human_observed_features_extract = np.delete(human_observed_features_extract,0,1)\n",
    "\n",
    "# Creating dictionary for img_id and corresponding features\n",
    "d = {}\n",
    "\n",
    "for i in range(1, 1027):\n",
    "    list_features = []\n",
    "    for j in human_observed_features_extract[i]:\n",
    "        list_features.append(j)\n",
    "    d[list_features[0]]  = list(map(float, list_features[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target for same pair\n",
    "\n",
    "input_appended_features_target_same = np.full((791,), 1, dtype=float)\n",
    "training_target_same = np.full((633,), 1, dtype=float)\n",
    "validation_target_same = GenerateTestingTarget(input_appended_features_target_same,len(training_target_same))\n",
    "testing_target_same = GenerateTestingTarget(input_appended_features_target_same, (len(training_target_same)+len(validation_target_same)))\n",
    "\n",
    "# Target for diffn pair\n",
    "input_appended_features_target_diffn = np.full((791,), 0, dtype=float)\n",
    "\n",
    "training_target_diffn = GenerateTrainingTarget(input_appended_features_target_diffn, 80)\n",
    "validation_target_diffn = GenerateTestingTarget(input_appended_features_target_diffn,len(training_target_diffn))\n",
    "testing_target_diffn = GenerateTestingTarget(input_appended_features_target_diffn, (len(training_target_diffn)+len(validation_target_diffn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracting the two img_id features to create input for same pairs\n",
    "\n",
    "input_same_pairs = input_same_pairs[:,[0,1]]\n",
    "\n",
    "input_subtracted_features_same_matrix = []\n",
    "for i in range(0,791):\n",
    "    img_id1_features = []\n",
    "    input_subtracted_features = []\n",
    "    for j in input_same_pairs[i]:\n",
    "        img_id1_features.append(d[j])\n",
    "    input_subtracted_features = [abs(a_i - b_i) for a_i, b_i in zip(img_id1_features[0], img_id1_features[1])]\n",
    "    input_subtracted_features_same_matrix.append(input_subtracted_features)\n",
    "\n",
    "input_diffn_pairs = input_diffn_pairs[:,[0,1]]\n",
    "input_subtracted_features_diffn_matrix = []\n",
    "for i in range(0,791):\n",
    "    img_id1_features = []\n",
    "    input_subtracted_features = []\n",
    "    for j in input_diffn_pairs[i]:\n",
    "        img_id1_features.append(d[j])\n",
    "    input_subtracted_features = [abs(a_i - b_i) for a_i, b_i in zip(img_id1_features[0], img_id1_features[1])]\n",
    "    input_subtracted_features_diffn_matrix.append(input_subtracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into Training, validation and testing\n",
    "\n",
    "# Input for same pair\n",
    "\n",
    "training_same_feature_matrix_subtracted = np.array(GenerateTrainingDataMatrix(input_subtracted_features_same_matrix, 80))\n",
    "validation_same_feature_matrix_subtracted = np.array(GenerateValidationDataMatrix(input_subtracted_features_same_matrix, 633))\n",
    "testing_same_feature_matrix_subtracted = np.array(GenerateValidationDataMatrix(input_subtracted_features_same_matrix,(633+79)))\n",
    "\n",
    "# Input for diffn pair\n",
    "\n",
    "training_diffn_feature_matrix_subtracted = np.array(GenerateTrainingDataMatrix(input_subtracted_features_diffn_matrix, 80))\n",
    "validation_diffn_feature_matrix_subtracted = np.array(GenerateValidationDataMatrix(input_subtracted_features_diffn_matrix, 633))\n",
    "testing_diffn_feature_matrix_subtracted = np.array(GenerateValidationDataMatrix(input_subtracted_features_diffn_matrix,(633+79)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final input using subtraction (Combine Same and different pairs input)\n",
    "\n",
    "input_data_training_subtracted = np.concatenate([training_same_feature_matrix_subtracted, training_diffn_feature_matrix_subtracted])\n",
    "input_data_testing_subtracted = np.concatenate([testing_same_feature_matrix_subtracted, testing_diffn_feature_matrix_subtracted])\n",
    "input_data_validation_subtracted = np.concatenate([validation_same_feature_matrix_subtracted,validation_diffn_feature_matrix_subtracted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the two img_id features to create input for same pairs\n",
    "\n",
    "input_same_pairs = input_same_pairs[:,[0,1]]\n",
    "\n",
    "input_appended_features_same_matrix = []\n",
    "for i in range(0,791):\n",
    "    input_appended_features = []\n",
    "    for j in input_same_pairs[i]:\n",
    "        for item in d[j]:\n",
    "            temp = []\n",
    "            temp = float(item)\n",
    "            input_appended_features.append(temp)\n",
    "    input_appended_features_same_matrix.append(input_appended_features)\n",
    "\n",
    "# Appending the two img_id features to create input for diffn pairs\n",
    "\n",
    "input_diffn_pairs = input_diffn_pairs[:,[0,1]]\n",
    "\n",
    "input_appended_features_diffn_matrix = []\n",
    "for i in range(0,791):\n",
    "    input_appended_features = []\n",
    "    for j in input_diffn_pairs[i]:\n",
    "        for item in d[j]:\n",
    "            temp = []\n",
    "            temp = float(item)\n",
    "            input_appended_features.append(temp)\n",
    "    input_appended_features_diffn_matrix.append(input_appended_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into Training, validation and testing\n",
    "\n",
    "# Input for same pair\n",
    "\n",
    "training_same_feature_matrix = np.array(GenerateTrainingDataMatrix(input_appended_features_same_matrix, 80))\n",
    "validation_same_feature_matrix = np.array(GenerateValidationDataMatrix(input_appended_features_same_matrix, 633))\n",
    "testing_same_feature_matrix = np.array(GenerateValidationDataMatrix(input_appended_features_same_matrix,(633+79)))\n",
    "\n",
    "# Input for diffn pair\n",
    "\n",
    "training_diffn_feature_matrix = np.array(GenerateTrainingDataMatrix(input_appended_features_diffn_matrix, 80))\n",
    "validation_diffn_feature_matrix = np.array(GenerateValidationDataMatrix(input_appended_features_diffn_matrix, 633))\n",
    "testing_diffn_feature_matrix = np.array(GenerateValidationDataMatrix(input_appended_features_diffn_matrix,(633+79)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final input using concatenation (Combine Same and different pairs input)\n",
    "\n",
    "input_data_training = np.concatenate([training_same_feature_matrix, training_diffn_feature_matrix])\n",
    "input_data_testing = np.concatenate([testing_same_feature_matrix, testing_diffn_feature_matrix])\n",
    "input_data_validation = np.concatenate([validation_same_feature_matrix,validation_diffn_feature_matrix])\n",
    "\n",
    "output_data_training = np.concatenate([training_target_same,training_target_diffn])\n",
    "output_data_testing = np.concatenate([testing_target_same,testing_target_diffn])\n",
    "output_data_validation = np.concatenate([validation_target_same,validation_target_diffn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling and partitioning data for Feature concatenated HOD\n",
    "\n",
    "input_appended_features_shuffled, input_appended_features_target_shuffled = shuffleDataSet(input_appended_features_same_matrix, input_appended_features_diffn_matrix, input_appended_features_target_same, input_appended_features_target_diffn)\n",
    "\n",
    "input_appended_features_shuffled_tr, input_appended_features_shuffled_val, input_appended_features_shuffled_test = partitionDataSet(input_appended_features_shuffled, 80)\n",
    "\n",
    "input_appended_target_shuffled_tr, input_appended_target_shuffled_val, input_appended_target_shuffled_test = partitionDataSet(input_appended_features_target_shuffled,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling and partitioning data for Feature subtracted HOD\n",
    "\n",
    "input_subtracted_features_shuffled, input_subtracted_features_target_shuffled = shuffleDataSet(input_subtracted_features_same_matrix, input_subtracted_features_diffn_matrix, input_appended_features_target_same, input_appended_features_target_diffn)\n",
    "\n",
    "input_subtracted_features_shuffled_tr, input_subtracted_features_shuffled_val, input_subtracted_features_shuffled_test = partitionDataSet(input_subtracted_features_shuffled,80)\n",
    "\n",
    "input_subtracted_target_shuffled_tr, input_subtracted_target_shuffled_val, input_subtracted_target_shuffled_test = partitionDataSet(input_subtracted_features_target_shuffled,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for Human Observed Features Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "------------ Linear Regression for Input Concatenated features----------\n",
      "Entered GD\n",
      "E_rms Training   = 0.6099\n",
      "E_rms Validation = 0.70453\n",
      "E_rms Testing    = 0.70382\n",
      "Training accuracy = 50.0\n",
      "Validation accuracy = 50.0\n",
      "Testing accuracy = 50.0\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"------------ Linear Regression for Input Concatenated features----------\")\n",
    "LinearRegressionMethod(input_data_training,input_data_validation,input_data_testing,output_data_training, output_data_validation, output_data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "------------ Linear Regression for Input subtracted features--------\n",
      "Entered GD\n",
      "E_rms Training   = 0.52303\n",
      "E_rms Validation = 0.56265\n",
      "E_rms Testing    = 0.60847\n",
      "Training accuracy = 50.0\n",
      "Validation accuracy = 50.0\n",
      "Testing accuracy = 50.0\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"------------ Linear Regression for Input subtracted features--------\")\n",
    "LinearRegressionMethod(input_data_training_subtracted, input_data_validation_subtracted,input_data_testing_subtracted,output_data_training, output_data_validation, output_data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "------------ Linear Regression for Input Concatenated features SHUFFLED DATA SET----------\n",
      "Entered GD\n",
      "E_rms Training   = 0.59501\n",
      "E_rms Validation = 0.60971\n",
      "E_rms Testing    = 0.5734\n",
      "Training accuracy = 49.13112164296998\n",
      "Validation accuracy = 52.53164556962025\n",
      "Testing accuracy = 54.48717948717948\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"------------ Linear Regression for Input Concatenated features SHUFFLED DATA SET----------\")\n",
    "\n",
    "input_appended_target_shuffled_tr = input_appended_target_shuffled_tr.reshape(len(input_appended_target_shuffled_tr),)\n",
    "LinearRegressionMethod(input_appended_features_shuffled_tr,input_appended_features_shuffled_val,input_appended_features_shuffled_test,input_appended_target_shuffled_tr, input_appended_target_shuffled_val, input_appended_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "------------ Linear Regression for Input subtracted features SHUFFLED DATA SET--------\n",
      "Entered GD\n",
      "E_rms Training   = 0.5346\n",
      "E_rms Validation = 0.53069\n",
      "E_rms Testing    = 0.53752\n",
      "Training accuracy = 50.71090047393365\n",
      "Validation accuracy = 47.46835443037975\n",
      "Testing accuracy = 47.43589743589743\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"------------ Linear Regression for Input subtracted features SHUFFLED DATA SET--------\")\n",
    "\n",
    "input_subtracted_target_shuffled_tr = input_subtracted_target_shuffled_tr.reshape(len(input_subtracted_target_shuffled_tr),)\n",
    "LinearRegressionMethod(input_subtracted_features_shuffled_tr, input_subtracted_features_shuffled_val, input_subtracted_features_shuffled_test,input_subtracted_target_shuffled_tr, input_subtracted_target_shuffled_val, input_subtracted_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Human Observed Features Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Logistic Regression Implemented for concatenated Features for HOD SHUFFLED DATASET---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a244b94e07f0450da1c54290f75be274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_rms Training   = [0.69272]\n",
      "E_rms Validation = [0.6802]\n",
      "E_rms Testing    = [0.67246]\n",
      "Training accuracy = 49.13112164296998\n",
      "Validation accuracy = 52.53164556962025\n",
      "Testing accuracy = 54.14012738853503\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------Logistic Regression Implemented for concatenated Features for HOD SHUFFLED DATASET---------------\")\n",
    "input_appended_target_shuffled_tr = input_appended_target_shuffled_tr.reshape(len(input_appended_target_shuffled_tr),1)\n",
    "LogisticRegressionOwnImpl(input_appended_features_shuffled_tr, input_appended_features_shuffled_val, \n",
    "                          input_appended_features_shuffled_test,input_appended_target_shuffled_tr, \n",
    "                          input_appended_target_shuffled_val, input_appended_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Logistic Regression Implemented for subtracted Features for HOD SHUFFLED DATASET---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e93d13a88c4db380eaf787e24f3e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_rms Training   = [0.72744]\n",
      "E_rms Validation = [0.76316]\n",
      "E_rms Testing    = [0.75839]\n",
      "Training accuracy = 50.71090047393365\n",
      "Validation accuracy = 47.46835443037975\n",
      "Testing accuracy = 47.13375796178344\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------Logistic Regression Implemented for subtracted Features for HOD SHUFFLED DATASET---------------\")\n",
    "input_subtracted_target_shuffled_tr = input_subtracted_target_shuffled_tr.reshape(len(input_subtracted_target_shuffled_tr),1)\n",
    "LogisticRegressionOwnImpl(input_subtracted_features_shuffled_tr, input_subtracted_features_shuffled_val, \n",
    "                     input_subtracted_features_shuffled_test,input_subtracted_target_shuffled_tr, \n",
    "                     input_subtracted_target_shuffled_val, input_subtracted_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Logistic Regression Implemented for concatenated Features for HOD using SKLEARN---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoMo\\Anaconda3\\envs\\ML Env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_rms Training   = 0.27823\n",
      "E_rms Validation = 0.13779\n",
      "E_rms Testing    = 0.08006\n",
      "Training Accuracy = 92.25908372827804\n",
      "Validation Accuracy = 98.10126582278481\n",
      "Testing Accuracy  = 99.35897435897436\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------Logistic Regression Implemented for concatenated Features for HOD using SKLEARN---------------\")\n",
    "LogisticMethod(input_data_training, input_data_validation, input_data_testing,output_data_training, output_data_validation, output_data_testing)\n",
    "\n",
    "# LogisticRegressionGD(input_data_training, input_data_validation, input_data_testing,output_data_training, output_data_validation, output_data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "------------ Logistic Regression for Input subtracted features for HOD using SKLEARN--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoMo\\Anaconda3\\envs\\ML Env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_rms Training   = 0.45665\n",
      "E_rms Validation = 0.29767\n",
      "E_rms Testing    = 0.37553\n",
      "Training Accuracy = 79.14691943127961\n",
      "Validation Accuracy = 91.13924050632912\n",
      "Testing Accuracy  = 85.8974358974359\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"------------ Logistic Regression for Input subtracted features for HOD using SKLEARN--------\")\n",
    "LogisticMethod(input_data_training_subtracted, input_data_validation_subtracted, input_data_testing_subtracted,output_data_training, output_data_validation, output_data_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Human Observed Features Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Neural Networks for HOF input concatenated features SHUFFLED DATA SET------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb461edf1d0470c92a8d4e72d7c4278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 72  Correct :85\n",
      "Testing Accuracy: 54.14012738853503\n"
     ]
    }
   ],
   "source": [
    "print(\"------------Neural Networks for HOF input concatenated features SHUFFLED DATA SET------------------------\")\n",
    "\n",
    "training_accuracy, predictedTestLabel = NeuralNetworkMethod(18, input_appended_features_shuffled_tr,\n",
    "                        input_appended_features_shuffled_test,input_appended_target_shuffled_tr,\n",
    "                        input_appended_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Neural Networks for HOF input subtracted features SHUFFLED DATA SET------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6f5afcc47b4ab39926eb34a1114d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 78  Correct :79\n",
      "Testing Accuracy: 50.318471337579616\n"
     ]
    }
   ],
   "source": [
    "print(\"------------Neural Networks for HOF input subtracted features SHUFFLED DATA SET------------------------\")\n",
    "\n",
    "training_accuracy, predictedTestLabel = NeuralNetworkMethod(9, input_subtracted_features_shuffled_tr, \n",
    "                                                            input_subtracted_features_shuffled_test,\n",
    "                                                            input_subtracted_target_shuffled_tr, \n",
    "                                                            input_subtracted_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSC Data Set Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoMo\\Anaconda3\\envs\\ML Env\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: 'U' mode is deprecated\n",
      "  \n",
      "C:\\Users\\MoMo\\Anaconda3\\envs\\ML Env\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: 'U' mode is deprecated\n",
      "  \n",
      "C:\\Users\\MoMo\\Anaconda3\\envs\\ML Env\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: 'U' mode is deprecated\n"
     ]
    }
   ],
   "source": [
    "GSC_file_path_input_same_pairs = r'GSC_same_pairs.csv'\n",
    "GSC_file_path_input_diffn_pairs = r'GSC_diffn_pairs.csv'\n",
    "GSC_file_path_GSC_features = r'GSC-Features.csv'\n",
    "\n",
    "# Raw data \n",
    "GSC_input_same_pairs, GSC_input_diffn_pairs, GSC_features_extract = import_data(GSC_file_path_input_diffn_pairs, GSC_file_path_input_same_pairs, GSC_file_path_GSC_features)\n",
    "\n",
    "# Deleting headings from raw data\n",
    "GSC_input_same_pairs = np.delete(GSC_input_same_pairs,0, 0)\n",
    "GSC_input_diffn_pairs = np.delete(GSC_input_diffn_pairs, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary for img_id and corresponding features\n",
    "GSC_d = {}\n",
    "\n",
    "for i in range(1, 14073):\n",
    "    list_features = []\n",
    "    for j in GSC_features_extract[i]:\n",
    "        list_features.append(j)\n",
    "    GSC_d[list_features[0]]  = list(map(float, list_features[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the two img_id features to create input for same pairs\n",
    "\n",
    "GSC_input_same_pairs = GSC_input_same_pairs[:,[0,1]]\n",
    "\n",
    "GSC_input_appended_features_same_matrix = []\n",
    "for i in range(0,71531):\n",
    "    GSC_input_appended_features = []\n",
    "    for j in GSC_input_same_pairs[i]:\n",
    "        for item in GSC_d[j]:\n",
    "            temp = []\n",
    "            temp = float(item)\n",
    "            GSC_input_appended_features.append(temp)\n",
    "    GSC_input_appended_features_same_matrix.append(GSC_input_appended_features)\n",
    "\n",
    "# Appending the two img_id features to create input for diffn pairs\n",
    "\n",
    "GSC_input_diffn_pairs = GSC_input_diffn_pairs[:,[0,1]]\n",
    "\n",
    "GSC_input_appended_features_diffn_matrix = []\n",
    "for i in range(0,71531):\n",
    "    GSC_input_appended_features = []\n",
    "    for j in GSC_input_diffn_pairs[i]:\n",
    "        for item in GSC_d[j]:\n",
    "            temp = []\n",
    "            temp = float(item)\n",
    "            GSC_input_appended_features.append(temp)\n",
    "    GSC_input_appended_features_diffn_matrix.append(GSC_input_appended_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target for same pair\n",
    "\n",
    "GSC_input_appended_features_target_same = np.full((71531,), 1, dtype=float)\n",
    "GSC_training_target_same = GenerateTrainingTarget(GSC_input_appended_features_target_same, 30)\n",
    "GSC_validation_target_same = GenerateTestingTarget(GSC_input_appended_features_target_same,len(GSC_training_target_same))\n",
    "GSC_testing_target_same = GenerateTestingTarget(GSC_input_appended_features_target_same, (len(GSC_training_target_same)+len(GSC_validation_target_same)))\n",
    "\n",
    "# Target for diffn pair\n",
    "GSC_input_appended_features_target_diffn = np.full((71531,), 0, dtype=float)\n",
    "\n",
    "GSC_training_target_diffn = np.array(GenerateTrainingTarget(GSC_input_appended_features_target_diffn, 30))\n",
    "GSC_validation_target_diffn = np.array(GenerateTestingTarget(GSC_input_appended_features_target_diffn,len(GSC_training_target_diffn)))\n",
    "GSC_testing_target_diffn = np.array(GenerateTestingTarget(GSC_input_appended_features_target_diffn, (len(GSC_training_target_diffn)+len(GSC_validation_target_diffn))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into Training, validation and testing\n",
    "\n",
    "# Input for same pair\n",
    "\n",
    "GSC_training_same_feature_matrix = np.array(GenerateTrainingDataMatrix(GSC_input_appended_features_same_matrix, 30))\n",
    "GSC_validation_same_feature_matrix = np.array(GenerateValidationDataMatrix(GSC_input_appended_features_same_matrix, len(GSC_training_same_feature_matrix)))\n",
    "GSC_testing_same_feature_matrix = np.array(GenerateValidationDataMatrix(GSC_input_appended_features_same_matrix,(len(GSC_training_same_feature_matrix)+len(GSC_validation_same_feature_matrix))))\n",
    "\n",
    "# Input for diffn pair\n",
    "\n",
    "GSC_training_diffn_feature_matrix = np.array(GenerateTrainingDataMatrix(GSC_input_appended_features_diffn_matrix, 30))\n",
    "GSC_validation_diffn_feature_matrix = np.array(GenerateValidationDataMatrix(GSC_input_appended_features_diffn_matrix, len(GSC_training_diffn_feature_matrix)))\n",
    "GSC_testing_diffn_feature_matrix = np.array(GenerateValidationDataMatrix(GSC_input_appended_features_diffn_matrix,(len(GSC_training_diffn_feature_matrix)+len(GSC_validation_same_feature_matrix))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final input and target using concatenation (Combine Same and different pairs input)\n",
    "\n",
    "GSC_input_data_training = np.concatenate([GSC_training_same_feature_matrix, GSC_training_diffn_feature_matrix])\n",
    "GSC_input_data_testing = np.concatenate([GSC_testing_same_feature_matrix, GSC_testing_diffn_feature_matrix])\n",
    "GSC_input_data_validation = np.concatenate([GSC_validation_same_feature_matrix,GSC_validation_diffn_feature_matrix])\n",
    "\n",
    "GSC_output_data_training = np.concatenate([GSC_training_target_same,GSC_training_target_diffn])\n",
    "GSC_output_data_testing = np.concatenate([GSC_testing_target_same,GSC_testing_target_diffn])\n",
    "GSC_output_data_validation = np.concatenate([GSC_validation_target_same,GSC_validation_target_diffn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracting the two img_id features to create input for same pairs\n",
    "\n",
    "GSC_input_subtracted_features_same_matrix = []\n",
    "for i in range(0,71531):\n",
    "    img_id1_features = []\n",
    "    input_subtracted_features = []\n",
    "    for j in GSC_input_same_pairs[i]:\n",
    "        img_id1_features.append(GSC_d[j])\n",
    "    input_subtracted_features = [abs(a_i - b_i) for a_i, b_i in zip(img_id1_features[0], img_id1_features[1])]\n",
    "    GSC_input_subtracted_features_same_matrix.append(input_subtracted_features)\n",
    "\n",
    "GSC_input_subtracted_features_diffn_matrix = []\n",
    "for i in range(0,71531):\n",
    "    img_id1_features = []\n",
    "    input_subtracted_features = []\n",
    "    for j in GSC_input_diffn_pairs[i]:\n",
    "        img_id1_features.append(GSC_d[j])\n",
    "    input_subtracted_features = [abs(a_i - b_i) for a_i, b_i in zip(img_id1_features[0], img_id1_features[1])]\n",
    "    GSC_input_subtracted_features_diffn_matrix.append(input_subtracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into Training, validation and testing\n",
    "\n",
    "# Input for same pair\n",
    "\n",
    "GSC_training_same_feature_matrix_subtracted = np.array(GenerateTrainingDataMatrix(GSC_input_subtracted_features_same_matrix, 30))\n",
    "GSC_validation_same_feature_matrix_subtracted = np.array(GenerateValidationDataMatrix(GSC_input_subtracted_features_same_matrix, len(GSC_training_same_feature_matrix)))\n",
    "GSC_testing_same_feature_matrix_subtracted = np.array(GenerateValidationDataMatrix(GSC_input_subtracted_features_same_matrix,(len(GSC_training_same_feature_matrix)+len(GSC_validation_same_feature_matrix))))\n",
    "\n",
    "# Input for diffn pair\n",
    "\n",
    "GSC_training_diffn_feature_matrix_subtracted = np.array(GenerateTrainingDataMatrix(GSC_input_subtracted_features_diffn_matrix, 30))\n",
    "GSC_validation_diffn_feature_matrix_subtracted = np.array(GenerateValidationDataMatrix(GSC_input_subtracted_features_diffn_matrix, len(GSC_training_diffn_feature_matrix)))\n",
    "GSC_testing_diffn_feature_matrix_subtracted = np.array(GenerateValidationDataMatrix(GSC_input_subtracted_features_diffn_matrix,(len(GSC_training_diffn_feature_matrix)+len(GSC_validation_diffn_feature_matrix))))\n",
    "\n",
    "# Final input using subtraction (Combine Same and different pairs input)\n",
    "\n",
    "GSC_input_data_training_subtracted = np.concatenate([GSC_training_same_feature_matrix_subtracted, GSC_training_diffn_feature_matrix_subtracted])\n",
    "GSC_input_data_testing_subtracted = np.concatenate([GSC_testing_same_feature_matrix_subtracted, GSC_testing_diffn_feature_matrix_subtracted])\n",
    "GSC_input_data_validation_subtracted = np.concatenate([GSC_validation_same_feature_matrix_subtracted,GSC_validation_diffn_feature_matrix_subtracted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling and partitioning data for Feature subtracted GSC\n",
    "\n",
    "GSC_input_subtracted_features_shuffled, GSC_input_subtracted_features_target_shuffled = shuffleDataSet(GSC_input_subtracted_features_same_matrix, GSC_input_subtracted_features_diffn_matrix, GSC_input_appended_features_target_same, GSC_input_appended_features_target_diffn)\n",
    "\n",
    "GSC_input_subtracted_features_shuffled_tr, GSC_input_subtracted_features_shuffled_val, GSC_input_subtracted_features_shuffled_test = partitionDataSet(GSC_input_subtracted_features_shuffled,30)\n",
    "\n",
    "GSC_input_subtracted_target_shuffled_tr, GSC_input_subtracted_target_shuffled_val, GSC_input_subtracted_target_shuffled_test = partitionDataSet(GSC_input_subtracted_features_target_shuffled,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling and partitioning data for Feature concatenate GSC\n",
    "\n",
    "GSC_input_appended_features_shuffled, GSC_input_appended_features_target_shuffled = shuffleDataSet(GSC_input_appended_features_same_matrix, GSC_input_appended_features_diffn_matrix, GSC_input_appended_features_target_same, GSC_input_appended_features_target_diffn)\n",
    "\n",
    "GSC_input_appended_features_shuffled_tr, GSC_input_appended_features_shuffled_val, GSC_input_appended_features_shuffled_test = partitionDataSet(GSC_input_appended_features_shuffled,30)\n",
    "\n",
    "GSC_input_appended_target_shuffled_tr, GSC_input_appended_target_shuffled_val, GSC_input_appended_target_shuffled_test = partitionDataSet(GSC_input_appended_features_target_shuffled,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for GSC Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegressionMethodforGSC(features, GSC_input_data_training, GSC_input_data_validation, GSC_input_data_testing,GSC_output_data_training, GSC_output_data_validation, GSC_output_data_testing):\n",
    "    # Linear Regression for Input Appended GSC dataset\n",
    "\n",
    "    kmeans = KMeans(n_clusters=10, random_state=0).fit(GSC_input_data_training)\n",
    "    GSC_Mu = kmeans.cluster_centers_\n",
    "    GSC_BigSigma     = GenerateBigSigma(GSC_input_data_training, GSC_Mu, True)\n",
    "    GSC_BigSigma = GSC_BigSigma + np.identity(features)\n",
    "    GSC_TRAINING_PHI = GetPhiMatrix(GSC_input_data_training, GSC_Mu, GSC_BigSigma, 42919)\n",
    "    GSC_VAL_PHI      = GetPhiMatrix(GSC_input_data_validation, GSC_Mu, GSC_BigSigma, 14306)\n",
    "    GSC_TEST_PHI     = GetPhiMatrix(GSC_input_data_testing, GSC_Mu, GSC_BigSigma, 14306) \n",
    "    GSC_W            = GetWeightsClosedForm(GSC_TRAINING_PHI,GSC_output_data_training,0.03)\n",
    "    GradientDescent(GSC_W, GSC_TRAINING_PHI, GSC_VAL_PHI, GSC_TEST_PHI, GSC_output_data_training, GSC_output_data_validation, GSC_output_data_testing)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "------------ Linear Regression for Input appended features GSC SHUFFLED DATA SET--------\n",
      "Entered GD\n",
      "E_rms Training   = 0.70799\n",
      "E_rms Validation = 0.70765\n",
      "E_rms Testing    = 0.70706\n",
      "Training accuracy = 49.8753465830984\n",
      "Validation accuracy = 49.92310918495736\n",
      "Testing accuracy = 50.00699007409478\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"------------ Linear Regression for Input appended features GSC SHUFFLED DATA SET--------\")\n",
    "\n",
    "GSC_input_appended_target_shuffled_tr = GSC_input_appended_target_shuffled_tr.reshape(len(GSC_input_appended_target_shuffled_tr),)\n",
    "LinearRegressionMethodforGSC(1024, GSC_input_appended_features_shuffled_tr, GSC_input_appended_features_shuffled_val, GSC_input_appended_features_shuffled_test,GSC_input_appended_target_shuffled_tr, GSC_input_appended_target_shuffled_val, GSC_input_appended_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "------------ Linear Regression for Input subtracted features SHUFFLED DATA SET--------\n",
      "(10, 512)\n",
      "(512, 512)\n",
      "(42919, 10)\n",
      "(14306, 10)\n",
      "(14306, 10)\n",
      "(10,)\n",
      "Entered GD\n",
      "E_rms Training   = 0.7085\n",
      "E_rms Validation = 0.70528\n",
      "E_rms Testing    = 0.70676\n",
      "Training accuracy = 49.803117500407744\n",
      "Validation accuracy = 50.25863274150706\n",
      "Testing accuracy = 50.04893051866349\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"------------ Linear Regression for Input subtracted features SHUFFLED DATA SET--------\")\n",
    "\n",
    "GSC_input_subtracted_target_shuffled_tr = GSC_input_subtracted_target_shuffled_tr.reshape(len(GSC_input_subtracted_target_shuffled_tr),)\n",
    "LinearRegressionMethodforGSC(512, GSC_input_subtracted_features_shuffled_tr, GSC_input_subtracted_features_shuffled_val, GSC_input_subtracted_features_shuffled_test,GSC_input_subtracted_target_shuffled_tr, GSC_input_subtracted_target_shuffled_val, GSC_input_subtracted_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for GSC Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "------------ Logistic Regression for Input Appended features--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoMo\\Anaconda3\\envs\\ML Env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_rms Training   = 0.09056\n",
      "E_rms Validation = 0.7113\n",
      "E_rms Testing    = 0.71301\n",
      "Training Accuracy = 99.17986952469711\n",
      "Validation Accuracy = 49.40584370194324\n",
      "Testing Accuracy  = 49.16119110862575\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"------------ Logistic Regression for Input Appended features--------\")\n",
    "LogisticMethod(GSC_input_data_training, GSC_input_data_validation, GSC_input_data_testing, GSC_output_data_training, GSC_output_data_validation, GSC_output_data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "------------ Logistic Regression for Input Subtracted features--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoMo\\Anaconda3\\envs\\ML Env\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_rms Training   = 0.31751\n",
      "E_rms Validation = 0.5111\n",
      "E_rms Testing    = 0.51157\n",
      "Training Accuracy = 89.91845293569432\n",
      "Validation Accuracy = 73.87809310778695\n",
      "Testing Accuracy  = 73.82916258912344\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"------------ Logistic Regression for Input Subtracted features--------\")\n",
    "LogisticMethod(GSC_input_data_training_subtracted, GSC_input_data_validation_subtracted, GSC_input_data_testing_subtracted, GSC_output_data_training, GSC_output_data_validation, GSC_output_data_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionOwnImplGSC(input_data_training, input_data_validation, input_data_testing, output_data_training, output_data_validation, output_data_testing):\n",
    "    \n",
    "    NUM_OF_EPOCHS = 500\n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    LR = 0.000001\n",
    "    W = np.random.rand(len(np.transpose(input_data_training)), 1) \n",
    "\n",
    "    for epoch in tqdm_notebook(range(NUM_OF_EPOCHS)):\n",
    "        \n",
    "        A = GetLogisticTarget(W, input_data_training)\n",
    "\n",
    "        Z = np.subtract(A,output_data_training)\n",
    "        DELTA_W = np.dot(np.transpose(input_data_training), Z)\n",
    "        W = W - np.dot(LR, DELTA_W)\n",
    "\n",
    "\n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "    TR_TEST_OUT   = np.dot(input_data_training,W) \n",
    "    Erms_TR       = GetErms(TR_TEST_OUT,output_data_training)\n",
    "    L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    L_Accuracy_TR = float(Erms_TR.split(',')[0])\n",
    "\n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "    VAL_TEST_OUT  = np.dot(input_data_validation,W) \n",
    "    Erms_Val      = GetErms(VAL_TEST_OUT,output_data_validation)\n",
    "    L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    L_Accuracy_Val = float(Erms_Val.split(',')[0])\n",
    "\n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "    TEST_OUT      = np.dot(input_data_testing,W) \n",
    "    Erms_Test = GetErms(TEST_OUT,output_data_testing)\n",
    "    L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "    L_Accuracy_Test = float(Erms_Test.split(',')[0])\n",
    "    \n",
    "    print (\"E_rms Training   = \" + str(np.around((L_Erms_TR),5)))\n",
    "    print (\"E_rms Validation = \" + str(np.around((L_Erms_Val),5)))\n",
    "    print (\"E_rms Testing    = \" + str(np.around((L_Erms_Test),5)))\n",
    "    print(\"Training accuracy = \" + str(L_Accuracy_TR))    \n",
    "    print(\"Validation accuracy = \" + str(L_Accuracy_Val))    \n",
    "    print(\"Testing accuracy = \" + str(L_Accuracy_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Logistic Regression Implemented for concatenated Features for GSC SHUFFLED DATASET---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d45454d31e49a0886c804f99bf36dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E_rms Training   = [2.04723]\n",
      "E_rms Validation = [2.08091]\n",
      "E_rms Testing    = [2.06709]\n",
      "Training accuracy = 19.695239870453644\n",
      "Validation accuracy = 19.802879910527054\n",
      "Testing accuracy = 19.537257094925206\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------Logistic Regression Implemented for concatenated Features for GSC SHUFFLED DATASET---------------\")\n",
    "GSC_input_appended_target_shuffled_tr = GSC_input_appended_target_shuffled_tr.reshape(len(GSC_input_appended_target_shuffled_tr),1)\n",
    "LogisticRegressionOwnImplGSC(GSC_input_appended_features_shuffled_tr, GSC_input_appended_features_shuffled_val, \n",
    "                          GSC_input_appended_features_shuffled_test,GSC_input_appended_target_shuffled_tr, \n",
    "                          GSC_input_appended_target_shuffled_val, GSC_input_appended_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Logistic Regression Implemented for subtracted Features for GSC SHUFFLED DATASET---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f17b5ac13f248f4bdbc782c907363f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E_rms Training   = [2.00995]\n",
      "E_rms Validation = [2.03787]\n",
      "E_rms Testing    = [2.02233]\n",
      "Training accuracy = 19.94221673384748\n",
      "Validation accuracy = 19.718999021389628\n",
      "Testing accuracy = 19.739969243673983\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------Logistic Regression Implemented for subtracted Features for GSC SHUFFLED DATASET---------------\")\n",
    "GSC_input_subtracted_target_shuffled_tr = GSC_input_subtracted_target_shuffled_tr.reshape(len(GSC_input_subtracted_target_shuffled_tr),1)\n",
    "LogisticRegressionOwnImplGSC(GSC_input_subtracted_features_shuffled_tr, GSC_input_subtracted_features_shuffled_val, \n",
    "                     GSC_input_subtracted_features_shuffled_test,GSC_input_subtracted_target_shuffled_tr, \n",
    "                     GSC_input_subtracted_target_shuffled_val, GSC_input_subtracted_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for GSC Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Neural Networks for GSC input concatenated features SHUFFLED DATA SET------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f45fffcc8947cdaf72c48edde4c03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 7145  Correct :7161\n",
      "Testing Accuracy: 50.055920592758284\n"
     ]
    }
   ],
   "source": [
    "print(\"------------Neural Networks for GSC input concatenated features SHUFFLED DATA SET------------------------\")\n",
    "\n",
    "training_accuracy, predictedTestLabel = NeuralNetworkMethodGSC(1024, GSC_input_appended_features_shuffled_tr,\n",
    "                        GSC_input_appended_features_shuffled_test,GSC_input_appended_target_shuffled_tr,\n",
    "                        GSC_input_appended_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Neural Networks for GSC input subtracted features SHUFFLED DATA SET------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8496e688476b455c8fe554fc44d8e491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 7171  Correct :7135\n",
      "Testing Accuracy: 49.87417866629386\n"
     ]
    }
   ],
   "source": [
    "print(\"------------Neural Networks for GSC input subtracted features SHUFFLED DATA SET------------------------\")\n",
    "\n",
    "training_accuracy, predictedTestLabel = NeuralNetworkMethodGSC(512, GSC_input_subtracted_features_shuffled_tr, \n",
    "                                                            GSC_input_subtracted_features_shuffled_test,\n",
    "                                                            GSC_input_subtracted_target_shuffled_tr, \n",
    "                                                            GSC_input_subtracted_target_shuffled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
